# CS 111 Project 2A

NAME: Joe Qian
EMAIL: qzy@g.ucla.edu
ID: 404816794
SLIPDAYS: 1

This is an implementation of Project 2B. The files included in this submission,
as reported by `tar tf *.tar.gz` are the following:

    README ............. This readme file
    Makefile ........... A Makefile to build the project
    lab2_list.c ........ Implementation of the linked list experiment
    SortedList.h ....... Provided interface for sorted doubly linked list
    SortedList.c ....... Implementation of the same, (same as lab2a)
    profile.out ........ CPU profiling data generated by google perf tools
    lab2_1.png ......... Plot 1
    lab2_2.png ......... Plot 2
    lab2_3.png ......... Plot 3
    lab2_4.png ......... Plot 4
    lab2_5.png ......... Plot 5
    lab2_1.gp .......... Gnuplot script to generate plot 1
    lab2_2.gp .......... Gnuplot script to generate plot 2
    lab2_3.gp .......... Gnuplot script to generate plot 3
    lab2_4.gp .......... Gnuplot script to generate plot 4
    lab2_5.gp .......... Gnuplot script to generate plot 5
    lab2_1.sh .......... Script to generate data for plot 1 and 2
    lab2_3.sh .......... Script to generate data for plot 3
    lab2_4.sh .......... Script to generate data for plot 4
    lab2_5.sh .......... Script to generate data for plot 5

Answers to the questions follow.

Question 2.3.1. In 1 and 2-thread list tests, I believe most of the cycles are
spent in list operations, including insert, lookup, and delete. This is because
I believe contention is low, so the productivity is high, and most of the time
is spent doing useful work. I believe most of the time/cycles are being spent in
spin-lock operations (__sync_lock_test_and_set and __sync_lock_release) due to
contention. In high-thread mutex tests, most of the time is spent in being
blocked and descheduled in a mutex operation.

Question 2.3.2. As expected, the lines of the code most cycles are spent are the
__sync_lock_test_and_set and __sync_lock_release operations. This operation
becomes expensive due to high contention for the single lock, causing each
thread to continually spin.

Question 2.3.3. The average lock-wait time rise dramatically because there is a
single lock and many threads competiting for access to this single lock. The
completion time rise less dramatically because even though each thread now takes
a longer time to complete, there are still overall more threads. It is possible
(and indeed observed) that the wait time per operation to go up faster (or
higher) than the completion time per operation because the latter is counted
once, but the former is counted multiple times. So if there are eight threads in
total, one has the lock, and seven is waiting, the waiting time is counted seven
times.

Question 2.3.4. The change in performance of the synchronized methods is obvious
when one looks at plots 4 and 5. As the number of threads increase, runs with
more sublists tend to degrade performance much more gently. When fixing the
number of threads and increasing sublists, performance is improved. This applies
for both POSIX mutexes and spinlocks, although the increase is more drastic for
spinlocks.
